[["index.html", "Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada 1 Introduction 1.1 Acknowledgement 1.2 Suggested Citation 1.3 Contact", " Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada Marcus Becker, Dr. David J. Huggard, Dr. David Roberts 2024-04-16 1 Introduction The Alberta Biodiversity Monitoring Institute (ABMI) has developed a monitoring program for mammals within the province of Alberta, Canada. This technical document contains information describing how the image data from remote camera traps deployed across the province is collected, processed, and analyzed. This document will be updated as necessary to reflect changes in available data, deployment protocols, and modeling procedures. Readers are also encouraged to visit the ABMI’s Biodiversity Browser for current mammal species results. 1.1 Acknowledgement We would like to acknowledge that this work would not be possible without the dedication of the ABMI staff, both past and present. Field staff, geospatial experts, taxonomists, image taggers, and ecologists have all been integral components to this work. 1.2 Suggested Citation Alberta Biodiversity Monitoring Institute. 2024. Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada. Alberta Biodiversity Monitoring Institute, Alberta, Canada. (https://abbiodiversity.github.io/MammalModels/). 1.3 Contact For all questions related to this document, the code therein, or acquiring raw image data, please contact Marcus Becker (marcus.becker@ualberta.ca). "],["data-collection.html", "2 Data Collection 2.1 Overview 2.2 Deployment Protocols 2.3 Camera Projects 2.4 Deployment Periods 2.5 Species Detections in Alberta", " 2 Data Collection 2.1 Overview The ABMI has been using remote cameras to monitor mammal species in the province since 2014. As of 2024-04-16, the ABMI has deployed X number of cameras across the province. 2.2 Deployment Protocols The ABMI’s remote camera deployment protocols can be accessed here: Terrestrial ABMI Autonomous Recording Unit (ARU) and Remote Camera Trap Protocols. 2.3 Camera Projects The following camera projects have been deployed by the ABMI: Organize by year or type of project? Use gt package? Note - would be nice to have a link to the WildTrax project for those projects that are published. Project No. Cameras Status Adopt-a-Camera 2017 83 Published - Map+Report Only Amphibian Monitoring 2020 19 Published - Map+Report Only Camera Model Comparison 2018 7 Active Camera Model Comparison 2019 14 Active Camera Model Comparison 2020 16 Active Camera Model Comparison 2021 19 Active Camera Model Comparison 2022 14 Active Citizen Science Monitoring 2016 23 Active Ecosystem Health 2014 22 Published - Map+Report Only Ecosystem Health 2015 552 Published - Map+Report Only Ecosystem Health 2016 624 Active Ecosystem Health 2017 723 Published - Map+Report Only Ecosystem Health 2018 712 Published - Map+Report Only Ecosystem Health 2019 418 Active Ecosystem Health 2020 175 Published - Map+Report Only Ecosystem Health 2021 86 Published - Map+Report Only Ecosystem Health 2022 213 Published - Map+Report Only Ecosystem Health 2023 134 Published - Map+Report Only Edge-Interior Surveys 2017 55 Active Height Comparison 2022 20 Active Northern Focal Areas 2019 201 Active Northern Focal Areas 2020 165 Active North Saskatchewan Monitoring 2018 22 Published - Map+Report Only Off-Grid Monitoring 2015 87 Published - Map+Report Only Off-Grid Monitoring 2017 63 Published - Map+Report Only Off-Grid Monitoring 2018 106 Published - Map+Report Only Operation Community Grassland 2015 12 Active OSM 2021 166 Published - Map+Report Only OSM 2022 203 Published - Map+Report Only Southern Focal Areas 2019 26 Active Southern Focal Areas 2021 27 Active Biodiversity Trajectories 2023 100 Active 2.3.1 Image Tagging Image tagging is done on the WildTrax platform. Raw data is also publicly available from the ABMI via this platform. The image tagging protocols used by the ABMI can be accessed here: There is a companion R package, wildRtrax, which is also publicly available and contains many useful functions for downloading and analyzing camera data hosted on the platform. Throughout this document, references will be made to functions within this package. Note: Would be nice to insert the wildRtrax package logo. 2.4 Deployment Periods Here we can include all those figures. Yay. Do we really want that? 2.5 Species Detections in Alberta The map below displays mammal species detections at ABMI Ecosystem Health (i.e., core grid) sites sampled with cameras between 2014 and 2022. Each site represented in the map below is sampled with four individual cameras placed 600-m apart in a square shape. Note that if a species is detected at at least one of the four cameras, the species is registered as an occurrence on the site map. (Note: Need to add EH 2023) Sampling Year: 2014 2015 2016 2017 2018 2019 2020 2021 2022 Species: "],["calculating-animal-density.html", "3 Calculating Animal Density 3.1 Overview 3.2 Basic Model 3.3 Time in Field-of-View 3.4 Time of Camera Operation 3.5 Effective Detection Distances 3.6 Calculating Density 3.7 Assumptions", " 3 Calculating Animal Density This chapter describes how the Time In Front of Camera (TIFC) method is used to calculate animal density with images collected from remote cameras. This method is also documented in Becker et al (2022). 3.1 Overview 3.1.1 Simple Explanation Density is the number of objects (trees, animals, etc.) per unit area. If a 100-m\\(^2\\) plot contains one tree, the density is 1 tree/100-m\\(^2\\), or 10,000 trees per km\\(^2\\) ( Figure ??). Similarly, if a camera has a field-of-view of 100-m\\(^2\\) and there is always one animal in the field-of-view for the whole time that the camera is operating, the density of that species is 1 animal per 100-m\\(^2\\), or 10,000 animals per km\\(^2\\). It doesn’t matter if the animal is moving around within the field-of-view, as long as it stays in the field-of-view for the whole time. On the other hand, if that camera only has an animal in the field-of-view 1/10,000 of the time that it is operating, there is 1/10,000 animal per 100-m\\(^2\\), or 1 animal per km-\\(^2\\). If the camera has two animals together for 1/10,000 of the time, this gives 2/10,000 animals per 100-m\\(^2\\), or 2 animals per km-\\(^2\\). This is how we use cameras to calculate density. For a given density of animals, this simple measure is independent of home range sizes or movement rates. If home ranges were twice as big, they would have to overlap twice as much to maintain the same density. Therefore, an individual would be in a particular camera’s field-of-view half as often (because its home range is bigger – it has more other places to be), but there would be twice as many individuals at that camera. If movement rates were twice as fast, an individual would pass by the camera twice as often, but would spend half as much time in the field-of-view (because it is moving faster). For the simple example above, there would be two visits to the camera each occupying 1/20,000 of the time the camera is operating, rather than one visit for 1/10,000 of the time. The other way of putting this is that only the total animal-time in the field-of-view matters, whether that comes from one long visit by one individual, several short visits by one individual, or several short visits each by a different individual. In all those cases, the density is the same; it is only the home range size and overlap and/or movement rates that are changing. Two features of cameras require us to do some additional data processing to use this simple density measure: Cameras do not survey fixed areas, unlike quadrats. The probability of an animal triggering the camera decreases with distance. We therefore have to estimate an effective detection distance (EDD) for the cameras, as is done for unlimited-distance point counts for birds or unlimited distance transect surveys. This effective distance can vary for different species, habitat types and time of year. Cameras take a series of images at discrete intervals, rather than providing a continuous record of how long an animal is in the field-of-view. The discrete intervals need to be converted to a continuous measure to show how long the animal was in the field-of-view, accounting for the possibility that a moderately long interval between images might be from an animal present but not moving much, and therefore not triggering the camera, versus an animal that left the field-of-view and returned. 3.2 Basic Model Here is the basic model (equation) 3.3 Time in Field-of-View 3.3.1 Probabilistic gaps From a pilot study, we determined that if there is a gap of less than 20 seconds between images of the same species at a camera, the animal is almost always still in the view (no evidence of it walking out and returning). Missing the odd time when it leaves the view for less than 20 seconds has little effect on estimates of the total time it is in the field-of-view. At the other end, if there is a gap of &gt;120 seconds between images of the same species, this almost always represented animals leaving and then returning (i.e., the animal is seen walking out of the field-of-view, then walking back in). Gaps of 20-120 seconds are uncertain. These relatively long periods when the animal could be in the field-of-view or not are important when estimating the total duration animals are in the field-of-view, and thus density. Using images from the 2015 ABMI Ecosystem Health project, we checked each 20-120 second gap in series’ of native mammals for evidence of the animal leaving and returning. We supplemented the 2015 data for less common species using data from 2016 and 2017. We looked at several images on either side of gaps of 20-120 seconds. In each sequence, the animal was designated as having left the field-of-view during the 20-120 second gap if there was clear evidence of it walking out of the field-of-view and then returning (or a different individual entering the field-of-view). If the animal stayed in one location within the field-of-view, or sequential images showed the animal in disconnected places (as often happens with smaller animals), the animal was assumed to have stayed. Through this tagging procedure, we obtained the following training data to estimate the probability of an animal leaving during a gaps between 20 and 120 seconds between images. The diff_time variable refers to the number of seconds between images and the left variable refers to whether or not the individual left the camera field-of-view and returned (‘Yes’) or showed in the field-of-view for the duration of the gap (‘No’). head(raw_gap_data, 10) ## # A tibble: 10 × 3 ## species_common_name diff_time left ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Snowshoe Hare 43 Yes ## 2 Canada Lynx 31 No ## 3 Canada Lynx 63 No ## 4 Canada Lynx 22 No ## 5 Canada Lynx 22 Yes ## 6 Moose 29 No ## 7 Moose 21 No ## 8 Moose 36 No ## 9 Snowshoe Hare 23 No ## 10 Canada Lynx 73 Yes Using White-tailed Deer as an example, we can visualize this data in Figure 3.1. In general, observations where the animal stayed in the field of view are more closely clustered around the 20 second mark, and observations where the animal left the field of view are more evenly spread across the range of gap lengths. Figure 3.1: White-tailed Deer. We used this data to develop models of the probability of a species leaving the field-of-view during a 20-120 second gap as a function of the gap duration. Smoothing splines were fit to the probability of leaving as a function of gap length, using a logit-linked binomial model. # Develop model for each species, then make predictions for each second along the 20-120 second span predictions &lt;- raw_gap_data |&gt; mutate(left = ifelse(left == &quot;Yes&quot;, 1, 0)) |&gt; group_by(species_common_name) |&gt; nest() |&gt; # Models and predictions mutate(model = map(.x = data, ~ smooth.spline(x = .$diff_time, y = .$left, df = 3)), pred = map(.x = model, ~ predict(., x = 20:120))) |&gt; select(species_common_name, pred) |&gt; unnest_wider(pred) |&gt; unnest(cols = c(x, y)) |&gt; rename(diff_time = x, pred = y) |&gt; ungroup() We can plot the model predictions for each value between the 20-120 second interval as follows (Figure 3.2). In the case of White-tailed Deer, the model predicts that there is ~40% chance that an individual would have left the field of view and returned during a gap of 20 seconds; similarly, the model predicts there is an over 80% chance that a gap of 120 seconds would mean the animal left the field of view and returned. Figure 3.2: Probability of leaving the field-of-view based on gap length between subsequent images for white-tailed deer. The 40% chance the individual left the camera field of view for gaps of 20 seconds implies that they may also have left in slightly shorter gaps, where we assume they all stayed. However, it makes little difference to the total duration to miss a few shorter gaps where the animals left (i.e., to add that extra little bit of time in), especially because we add time to the start and end of a series (next section). At the other end, there was evidence that animals left in about 80% of the 120 second gaps. Missing long periods when an individual is present in the field of view but not moving would lead to underestimates of their densities. For trend estimates and habitat modeling, we would have to assume that the prevalence of those misses does not change over time or by habitat type. Ideally, each gap between images would be checked to see if the animal left or not. However, the process of checking each gap is time-consuming. When we have direct information on gap-leaving, we define a series as any set of images separated by less than 120 seconds, unless the animal was observed to leave the field-of-view (in gaps of 20-120 seconds that were checked) (illustrated in 3.3). If it did leave in a gap, then the series ends at the preceding image, and a new series starts when the animal (or a different individual) returns in the subsequent image. The species is in the field-of-view from the first to the last image in the field-of-view (plus end buffers; described below). When we haven’t examined 20-120 second gaps, a series is all images separated by &lt;120 seconds. However, we then use the above models of gap-leaving probabilities to prorate the 20-120 second gaps for the probability that the species left for a gap of that length. Instead of the full 20-120 second gap length, we only add the duration of the gap x (1 – probability of leaving) to the total series length (illustrated in 3.3, bottom). For example, if there were 4 images separated by 10 seconds, 5 seconds, 60 seconds and 10 seconds, and the model for that species showed a 40% chance that it left in a gap of 60 seconds, then the total time in the field-of-view for that series is 10 + 5 + 60 x (1 - 0.4) + 10 = 61 seconds. (And, if there were an average of 1.25 individuals in those 4 images, the total animal-seconds would be 1.25 x 61 = 76.25 animal-seconds). Finally, we must consider the time before the first image and after the last image of a series, and time to allocate to single-image series’. We estimate that by calculating the average time between images in all series, separately by species (Table 3.1). This is typically 4-7 seconds for larger species and somewhat longer for small species. This time is added to the duration of each series, including to single-image series, which would otherwise have a duration of 0 seconds (striped sections in 3.3. The assumption is that the animal is in the field-of-view for half the average inter-photo time before the first image, and after the last image. (Although animals often appear to be further into the field-of-view when the first image is taken, and nearer the edge when the final one is taken, presumably reflecting a lag-time in the motion-detector – the assumption is that this averages to the average interval within series). When we have 20-120 second gaps that are using the probabilistic gap-leaving model, we add this extra time on the start and end of the whole series in the normal way, and we also added this time multiplied by the probability of leaving for each 20-120 second gap, to account for the cases where the animal would have left the field-of-view in those 20-120 second gaps and hence created another series. Figure 3.3: Illustration of how sequential individual images are converted to series, and how the species’ total time in the field-of-view is calculated for each series, when we have directly examined gaps of 20-120 seconds (top) or not examined them and used probabilistic model instead (bottom). Table 3.1: Average time between sequential images of the same species Species Average time between images (s) Snowshoe Hare 10.97 Fisher 9.81 Marten 8.45 Coyote 7.66 Black Bear 6.25 Gray Wolf 5.77 Pronghorn 5.43 White-tailed Deer 4.96 Canada Lynx 4.85 Mule Deer 4.76 Moose 4.53 Woodland Caribou 4.19 Elk (wapiti) 3.85 3.4 Time of Camera Operation 3.5 Effective Detection Distances One option to define the area surveyed by cameras is to define a fixed maximum distance, using a pole or other marker at 5m or 10m or whatever is appropriate. Animals beyond that distance are not counted. The assumption, which should be tested, is that all target species are detected if they are within that distance. The downside of this simple approach is that it excludes data from animals detected in the potentially long tail of greater distances where they are partially detectable. The ABMI uses all images (unlimited distance), with a procedure to estimate the effective detection distance of cameras. “Effective detection distance” is the fixed distance that would give the same number of detections as observed if all animals up to that distance were perfectly detectable and none were detectable further away. This approach is used for any point counts or transects with unlimited distances or with distance bands beyond the distance of perfect detectability. In the ABMI protocol, we place a prominently coloured pole 5m from the camera. All native mammals are recorded as being closer than the pole or farther than the pole, with additional categories for animals that are uncertain (near 5m but not directly in line with the pole), investigating the pole, or investigating the camera. Simple geometry gives the effective detection distance from the proportion of locations that are &lt;5m away versus &gt;5m (excluding the uncertain and investigating images), which is shown in Equation (3.1). \\[\\begin{equation} EDD~(m) = \\frac{5}{sqrt(1-p_{&gt;5m})} \\tag{3.1} \\end{equation}\\] where p is the proportion of images with the species greater than 5-m away. The area surveyed by the camera then becomes as shown in Equation (3.2). \\[\\begin{equation} Surveyed~Area~(m^2)~=~\\frac{(π~*~EDD^2~*~angle)}{360} \\tag{3.2} \\end{equation}\\] where angle is the angle of the camera’s field-of-view in degrees (40° for the Reconyx PC900 model cameras that the ABMI uses)1. Detection distances are expected to differ for different species, by habitat types, and possibly by season (e.g., on snowpacks versus in summer shrubs). We therefore used the results to develop detection-distance models for eleven species groups and eight broad habitat types: deciduous forest, upland conifer forest, upland grass, shrub, lowland forest, wet grass, water and human footprint. BIC-based model selection examined seven models with those habitat types grouped into broader categories, and seven more that added a factor for season (winter = October 15 – April 14, summer = April 15 – October 14). Below we plot the estimated EDD for different species groups for each of the eight broad habitat types and two seasons. 3.6 Calculating Density Include how to calculate confidence intervals on density of areas. A final consideration is the sampling distribution of density estimates. Because individual cameras sample tiny areas compared to the home ranges of the species they survey, the resulting sampling distribution can be horrendous – the majority of cameras never detect the species at all (density = 0), a few cameras record the species passing by once or twice for brief periods (low densities), and a very few number of cameras record long durations as the animals forage, rest, or play in front of the camera, or revisit a favourite spot repeatedly (very high densities). Longer duration camera deployments can help smooth out some of that extreme variation, but ultimately large numbers of cameras are required for precise estimates. Appropriate questions, rigorous study designs, and modest expectations are required for camera-based studies. 3.7 Assumptions There are a number of strong assumptions involved in using this measure to estimate density of a species. A couple big assumptions are: The cameras are a random or otherwise representative sample of the area. The density estimate applies to the field-of-view of the cameras. To make inferences about a larger region, the cameras need to be surveying a random or representative (e.g., systematic, systematic-random, random stratified) sample of the larger region. In particular, if cameras are intentionally placed in areas where species are more common, such as game trails, then the density estimate only applies to those places, not to a larger region. Animals are not attracted to or repelled by the cameras (or posts used to deploy the cameras, etc). That also means that they do not spend more or less time in front of the camera because of the presence of the camera. The effect of lures or other attractants needs to be explicitly measured and accounted for. There are additional assumptions involved in the procedures to estimate effective detection distance, including an assumption that all animals within a certain distance of the camera are detected, and in converting the discrete images into time in field-of-view. These assumptions are discussed below. Because the world is complicated, assumptions are never met perfectly. The important thing is to consider – and, ideally, design auxiliary tests to measure – is whether the violations are serious enough to impact the answer to whatever question(s) the cameras are being used to answer. In many cases, absolute density estimates may not be accurate, but the results can still serve as a usable index of relative density, if assumptions are violated about equally in whatever units are being compared (habitat types, experimental treatments, years for long-term trend, etc). The PC900 model was discontinued by Reconyx and the ABMI now used the HP2X model. In Section X we discuss calibrations between the two camera models in order to combine data collected from both.↩︎ "],["testing-assumptions.html", "4 Testing Assumptions 4.1 Movement not affected by the camera 4.2 Representative sampling of microhabitats 4.3 Perfect detection near the camera", " 4 Testing Assumptions In this section we describe work that has been done to test the assumptions of the TIFC method. 4.1 Movement not affected by the camera 4.2 Representative sampling of microhabitats 4.3 Perfect detection near the camera "],["protocol-calibrations.html", "5 Protocol Calibrations 5.1 Lure 5.2 Deployment Height 5.3 Camera Model 5.4 Camera Age 5.5 Game Trails", " 5 Protocol Calibrations In this section we describe corrections to common protocol differences. 5.1 Lure For many species, and carnivores in particular, detections at randomly located camera sites can be very low. Lure or other attractants are often used to increase the number of detections at cameras, which can help reduce the high inherent measurement error of cameras. However, lures clearly violate the assumption that animal movement is not influenced by the camera deployment. The ABMI deploys both lured and unlured cameras in a paired design to calibrate lure effects for each species. Each site uses four cameras spaced 600 m apart, two of which were lured. This design allows for a simple paired comparison of species’ occurrences and time in field-of-view. Direct comparisons of lured and unlured deployments are not confounded by geographic location, year, or, with enough samples, microhabitats. We examined results for common species at 558 core sites from both lured and unlured cameras (992 of each, with two pairs at most sites) with the same total operating times. We summarized the ratio of lured:unlured results by species in three measures: (1) Occurrence (presence/absence at the camera over the entire deployment time); (2) Density given Occurrence (density at only cameras where the species was present); and (3) Total Density (the product of occurrence and density given occurrence, that is, our density estimates as described in previous sections). We used bootstrapping to calculate confidence intervals for each mean ratio, with site as the re-sampling unit2. Table ?? displays the results across all three measures for 14 species with sufficient data. Figure (fig:lure-plot) displays the Total Density results for those same 14 species, differentiating between the carnivores and non-carnivores. #uyivxiuhiz table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #uyivxiuhiz thead, #uyivxiuhiz tbody, #uyivxiuhiz tfoot, #uyivxiuhiz tr, #uyivxiuhiz td, #uyivxiuhiz th { border-style: none; } #uyivxiuhiz p { margin: 0; padding: 0; } #uyivxiuhiz .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uyivxiuhiz .gt_caption { padding-top: 4px; padding-bottom: 4px; } #uyivxiuhiz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uyivxiuhiz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #uyivxiuhiz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uyivxiuhiz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uyivxiuhiz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uyivxiuhiz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uyivxiuhiz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uyivxiuhiz .gt_column_spanner_outer:first-child { padding-left: 0; } #uyivxiuhiz .gt_column_spanner_outer:last-child { padding-right: 0; } #uyivxiuhiz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #uyivxiuhiz .gt_spanner_row { border-bottom-style: hidden; } #uyivxiuhiz .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #uyivxiuhiz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uyivxiuhiz .gt_from_md > :first-child { margin-top: 0; } #uyivxiuhiz .gt_from_md > :last-child { margin-bottom: 0; } #uyivxiuhiz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uyivxiuhiz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #uyivxiuhiz .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #uyivxiuhiz .gt_row_group_first td { border-top-width: 2px; } #uyivxiuhiz .gt_row_group_first th { border-top-width: 2px; } #uyivxiuhiz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uyivxiuhiz .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #uyivxiuhiz .gt_first_summary_row.thick { border-top-width: 2px; } #uyivxiuhiz .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uyivxiuhiz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uyivxiuhiz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uyivxiuhiz .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #uyivxiuhiz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uyivxiuhiz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uyivxiuhiz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uyivxiuhiz .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uyivxiuhiz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uyivxiuhiz .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uyivxiuhiz .gt_left { text-align: left; } #uyivxiuhiz .gt_center { text-align: center; } #uyivxiuhiz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uyivxiuhiz .gt_font_normal { font-weight: normal; } #uyivxiuhiz .gt_font_bold { font-weight: bold; } #uyivxiuhiz .gt_font_italic { font-style: italic; } #uyivxiuhiz .gt_super { font-size: 65%; } #uyivxiuhiz .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #uyivxiuhiz .gt_asterisk { font-size: 100%; vertical-align: 0; } #uyivxiuhiz .gt_indent_1 { text-indent: 5px; } #uyivxiuhiz .gt_indent_2 { text-indent: 10px; } #uyivxiuhiz .gt_indent_3 { text-indent: 15px; } #uyivxiuhiz .gt_indent_4 { text-indent: 20px; } #uyivxiuhiz .gt_indent_5 { text-indent: 25px; } Species Occurrence Density|Occurrence Total Density White-tailed Deer 1.03 0.93 0.96 Moose 1.08 1.07 1.15 Elk 1.13 1.30 1.46 Mule Deer 1.02 1.74 1.77 Canada Lynx 1.47 1.39 2.05 Woodland Caribou 1.27 2.03 2.59 Black Bear 1.35 2.49 3.36 Marten 2.51 1.49 3.73 Cougar 2.35 1.65 3.88 Grizzly Bear 1.63 2.46 4.01 Coyote 1.40 2.92 4.08 Gray Wolf 1.80 3.02 5.43 Red Fox 2.12 3.36 7.13 Fisher 4.55 2.38 10.83 Figure 5.1: Lure effect on Total Density The ratio of the mean lured:unlured values for moose occurrence was 1.07 (90% CI: 0.99–1.16), 1.17 (0.92–1.5) for density given occurrence, and 1.26 (0.98–1.62) for total density. We found more substantial positive effects of lure for other species, particularly fisher (Pekania pennanti), red fox (Vulpes vulpes), and gray wolf (Canis lupus) (Appendix S5). For these carnivore species, a larger proportion of the effect on total density came from the density given occurrence component, indicating that lure was primarily effective for increasing the time animals spend in the camera field-of-view. Occurrence increases due to lure tended to be smaller, which suggests that animals are not being drawn in from large distances. We use the total density ratios to correct the estimates of densities at lured cameras to an unlured density equivalent. 5.2 Deployment Height 5.3 Camera Model 5.4 Camera Age 5.5 Game Trails Calculating Lure Effects - This script demonstrates how lure effects are calculated using the paired ABMI camera design.↩︎ "],["sharing-your-book.html", "6 Sharing your book 6.1 Publishing 6.2 404 pages 6.3 Metadata for sharing 6.4 Footnotes 6.5 Citations", " 6 Sharing your book 6.1 Publishing HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html 6.2 404 pages By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you’d like to customize your 404 page instead of using the default, you may add either a _404.Rmd or _404.md file to your project root and use code and/or Markdown syntax. 6.3 Metadata for sharing Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the index.Rmd YAML. To setup, set the url for your book and the path to your cover-image file. Your book’s title and description are also used. This gitbook uses the same social sharing data across all chapters in your book- all links shared will look the same. Specify your book’s source repository on GitHub using the edit key under the configuration options in the _output.yml file, which allows users to suggest an edit by linking to a chapter’s source file. Read more about the features of this output format here: https://pkgs.rstudio.com/bookdown/reference/gitbook.html Or use: ?bookdown::gitbook 6.4 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 3. 6.5 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2022) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
