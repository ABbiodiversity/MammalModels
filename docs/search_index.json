[["index.html", "Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada 1 Introduction 1.1 Acknowledgement 1.2 Suggested Citation 1.3 Contact", " Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada Marcus Becker, Dr. David J. Huggard, Dr. David Roberts 2024-01-03 1 Introduction The Alberta Biodiversity Monitoring Institute (ABMI) has developed a monitoring program for mammals within the province of Alberta, Canada. This technical document contains information describing how the image data from remote camera traps deployed across the province is collected, processed, and analyzed. This document will be updated as necessary to reflect changes in available data, deployment protocols, and modeling procedures. Readers are also encouraged to visit the ABMI’s Biodiversity Browser for current mammal species results. 1.1 Acknowledgement We would like to acknowledge that this work would not be possible without the dedication of the ABMI staff, both past and present. Field staff, geospatial experts, taxonomists, image taggers, and ecologists have all been integral components to this work. 1.2 Suggested Citation Alberta Biodiversity Monitoring Institute. 2024. Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada. Alberta Biodiversity Monitoring Institute, Alberta, Canada. (https://abbiodiversity.github.io/MammalModels/). 1.3 Contact For all questions related to this document, the code therein, or acquiring raw image data, please contact Marcus Becker (marcus.becker@ualberta.ca). "],["data-collection.html", "2 Data Collection 2.1 ABMI use of remote cameras 2.2 Camera deployment protocols 2.3 List of ABMI camera projects 2.4 Species detections 2.5 Deployment duration lengths", " 2 Data Collection 2.1 ABMI use of remote cameras The ABMI has been using remote cameras to monitor mammal species in the province since 2014. As of 2024-01-03, the ABMI has deployed X number of cameras across the province. 2.2 Camera deployment protocols The ABMI’s remote camera deployment protocols can be accessed here: Terrestrial ABMI Autonomous Recording Unit (ARU) and Remote Camera Trap Protocols. 2.3 List of ABMI camera projects The following camera projects have been deployed by the ABMI: Organize by year or type of project? Use gt package? Note - would be nice to have a link to the WildTrax project for those projects that are published. Project No. Cameras Status Adopt-a-Camera 2017 83 Published - Map+Report Only Amphibian Monitoring 2020 19 Published - Map+Report Only Camera Model Comparison 2018 7 Active Camera Model Comparison 2019 14 Active Camera Model Comparison 2020 16 Active Camera Model Comparison 2021 19 Active Camera Model Comparison 2022 14 Active Citizen Science Monitoring 2016 23 Active Ecosystem Health 2014 22 Published - Map+Report Only Ecosystem Health 2015 552 Published - Map+Report Only Ecosystem Health 2016 624 Active Ecosystem Health 2017 723 Published - Map+Report Only Ecosystem Health 2018 712 Published - Map+Report Only Ecosystem Health 2019 418 Active Ecosystem Health 2020 175 Published - Map+Report Only Ecosystem Health 2021 86 Published - Map+Report Only Ecosystem Health 2022 213 Active Ecosystem Health 2023 134 Active Edge-Interior Surveys 2017 55 Active Height Comparison 2022 20 Active Northern Focal Areas 2019 201 Active Northern Focal Areas 2020 165 Active North Saskatchewan Monitoring 2018 22 Published - Map+Report Only Off-Grid Monitoring 2015 87 Published - Map+Report Only Off-Grid Monitoring 2017 63 Published - Map+Report Only Off-Grid Monitoring 2018 106 Published - Map+Report Only Operation Community Grassland 2015 12 Active OSM 2021 196 Published - Map+Report Only OSM 2022 203 Active Southern Focal Areas 2019 26 Active Southern Focal Areas 2021 27 Active Biodiversity Trajectories 2023 100 Active 2.3.1 Image Tagging Image tagging is done on the WildTrax platform. Raw data is also publicly available from the ABMI via this platform. The image tagging protocols used by the ABMI can be accessed here: There is a companion R package, wildRtrax, which is also publicly available and contains many useful functions for downloading and analyzing camera data hosted on the platform. Throughout this document, references will be made to functions within this package. Note: Would be nice to insert the wildRtrax package logo. 2.4 Species detections The map below displays mammal species detections at ABMI Ecosystem Health (i.e., core grid) sites sampled with cameras between 2014 and 2022. Each site represented in the map below is sampled with four individual cameras placed 600-m apart in a square shape. Note that if a species is detected at at least one of the four cameras, the species is registered as an occurrence on the site map. (Note: Need to add EH 2023) Sampling Year: 2014 2015 2016 2017 2018 2019 2020 2021 2022 Species: 2.5 Deployment duration lengths Here we can include all those figures. Yay. Do we really want that? "],["estimating-density.html", "3 Estimating Density 3.1 Overview 3.2 Probabilistic gaps", " 3 Estimating Density This chapter describes how the Time In Front of Camera (TIFC) method is used to calculate animal density using images collected from remote cameras. This method is also documented in Becker et al (2022). 3.1 Overview 3.1.1 Simple Explanation Density is the number of objects (trees, animals, etc.) per unit area. If a 100-m\\(^2\\) plot contains one tree, the density is 1 tree/100-m\\(^2\\), or 10,000 trees per km\\(^2\\) ( Figure ??). Similarly, if a camera has a field-of-view of 100-m\\(^2\\) and there is always one animal in the field-of-view for the whole time that the camera is operating, the density of that species is 1 animal per 100-m\\(^2\\), or 10,000 animals per km\\(^2\\). It doesn’t matter if the animal is moving around within the field-of-view, as long as it stays in the field-of-view for the whole time. On the other hand, if that camera only has an animal in the field-of-view 1/10,000 of the time that it is operating, there is 1/10,000 animal per 100-m\\(^2\\), or 1 animal per km-\\(^2\\). If the camera has two animals together for 1/10,000 of the time, this gives 2/10,000 animals per 100-m\\(^2\\), or 2 animals per km-\\(^2\\). This is how we use cameras to calculate density. For a given density of animals, this simple measure is independent of home range sizes or movement rates. If home ranges were twice as big, they would have to overlap twice as much to maintain the same density. Therefore, an individual would be in a particular camera’s field-of-view half as often (because its home range is bigger – it has more other places to be), but there would be twice as many individuals at that camera. If movement rates were twice as fast, an individual would pass by the camera twice as often, but would spend half as much time in the field-of-view (because it is moving faster). For the simple example above, there would be two visits to the camera each occupying 1/20,000 of the time the camera is operating, rather than one visit for 1/10,000 of the time. The other way of putting this is that only the total animal-time in the field-of-view matters, whether that comes from one long visit by one individual, several short visits by one individual, or several short visits each by a different individual. In all those cases, the density is the same; it is only the home range size and overlap and/or movement rates that are changing. Two features of cameras require us to do some additional data processing to use this simple density measure: Cameras do not survey fixed areas, unlike quadrats. The probability of an animal triggering the camera decreases with distance. We therefore have to estimate an effective detection distance (EDD) for the cameras, as is done for unlimited-distance point counts for birds or unlimited distance transect surveys. This effective distance can vary for different species, habitat types and time of year. Cameras take a series of images at discrete intervals, rather than providing a continuous record of how long an animal is in the field-of-view. The discrete intervals need to be converted to a continuous measure to show how long the animal was in the field-of-view, accounting for the possibility that a moderately long interval between images might be from an animal present but not moving much, and therefore not triggering the camera, versus an animal that left the field-of-view and returned. 3.1.2 Assumptions There are a number of strong assumptions involved in using this measure to estimate density of a species. A couple big assumptions are: The cameras are a random or otherwise representative sample of the area. The density estimate applies to the field-of-view of the cameras. To make inferences about a larger region, the cameras need to be surveying a random or representative (e.g., systematic, systematic-random, random stratified) sample of the larger region. In particular, if cameras are intentionally placed in areas where species are more common, such as game trails, then the density estimate only applies to those places, not to a larger region. Animals are not attracted to or repelled by the cameras (or posts used to deploy the cameras, etc). That also means that they do not spend more or less time in front of the camera because of the presence of the camera. The effect of lures or other attractants needs to be explicitly measured and accounted for. There are additional assumptions involved in the procedures to estimate effective detection distance, including an assumption that all animals within a certain distance of the camera are detected, and in converting the discrete images into time in field-of-view. These assumptions are discussed below. Because the world is complicated, assumptions are never met perfectly. The important thing is to consider – and, ideally, design auxiliary tests to measure – is whether the violations are serious enough to impact the answer to whatever question(s) the cameras are being used to answer. In many cases, absolute density estimates may not be accurate, but the results can still serve as a useable index of relative density, if assumptions are violated about equally in whatever units are being compared (habitat types, experimental treatments, years for long-term trend, etc). A final consideration is the sampling distribution of density estimates. Because individual cameras sample tiny areas compared to the home ranges of the species they survey, the resulting sampling distribution can be horrendous – the majority of cameras never detect the species at all (density = 0), a few cameras record the species passing by once or twice for brief periods (low densities), and a very few number of cameras record long durations as the animals forage, rest, or play in front of the camera, or revisit a favourite spot repeatedly (very high densities). Longer duration camera deployments can help smooth out some of that extreme variation, but ultimately large numbers of cameras are required for precise estimates. Appropriate questions, rigorous study designs, and modest expectations are required for camera-based studies. 3.2 Probabilistic gaps From a pilot study, we determined that if there is a gap of less than 20 seconds between images of the same species at a camera, the animal is almost always still in the view (no evidence of it walking out and returning). Missing the odd time when it leaves the view for less than 20 seconds has little effect on estimates of the total time it is in the field-of-view. At the other end, if there is a gap of &gt;120 seconds between images of the same species, this almost always represented animals leaving and then returning (i.e., the animal is seen walking out of the field-of-view, then walking back in). Gaps of 20-120 seconds are uncertain. These relatively long periods when the animal could be in the field-of-view or not are important when estimating the total duration animals are in the field-of-view, and thus density. Using images from the 2015 ABMI Ecosystem Health project, we checked each 20-120 second gap in series’ of native mammals for evidence of the animal leaving and returning. We supplemented the 2015 data for less common species using data from 2016 and 2017. We looked at several images on either side of gaps of 20-120 seconds. In each sequence, the animal was designated as having left the field-of-view during the 20-120 second gap if there was clear evidence of it walking out of the field-of-view and then returning (or a different individual entering the field-of-view). If the animal stayed in one location within the field-of-view, or sequential images showed the animal in disconnected places (as often happens with smaller animals), the animal was assumed to have stayed. Through this tagging procedure, we obtained the following training data to estimate the probability of an animal leaving during a gaps between 20 and 120 seconds between images. The diff_time variable refers to the number of seconds between images and the left variable refers to whether or not the individual left the camera field-of-view and returned (‘Yes’) or showed in the field-of-view for the duration of the gap (‘No’). head(raw_gap_data, 10) ## # A tibble: 10 × 3 ## species_common_name diff_time left ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Snowshoe Hare 43 Yes ## 2 Canada Lynx 31 No ## 3 Canada Lynx 63 No ## 4 Canada Lynx 22 No ## 5 Canada Lynx 22 Yes ## 6 Moose 29 No ## 7 Moose 21 No ## 8 Moose 36 No ## 9 Snowshoe Hare 23 No ## 10 Canada Lynx 73 Yes Using White-tailed Deer as an example, we can visualize this data in Figure 3.1. In general, observations where the animal stayed in the field of view are more closely clustered around the 20 second mark, and observations where the animal left the field of view are more evenly spread across the range of gap lengths. # Plot of checked gaps for wtd raw_gap_data |&gt; mutate(left = as.factor(left)) |&gt; filter(species_common_name == &quot;White-tailed Deer&quot;) |&gt; ggplot(aes(x = diff_time, y = left, color = left)) + geom_jitter(height = 0.15, size = 2, alpha = 0.2) + scale_color_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;)) + theme_abmi() + # Custom ABMI theme scale_y_discrete(labels = c(&quot;Stayed&quot;, &quot;Left&quot;)) + scale_x_continuous(breaks = seq(20, 120, by = 10), limits = c(20,120)) + labs(y = &quot;&quot;, x = &quot;Length of gap between images (seconds)&quot;) + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(size = 11), axis.text.y = element_text(size = 13, face = &quot;bold&quot;)) Figure 3.1: White-tailed Deer. We used this data to develop models of the probability of a species leaving the field-of-view during a 20-120 second gap as a function of the gap duration. Smoothing splines were fit to the probability of leaving as a function of gap length, using a logit-linked binomial model. # Develop model for each species, then make predictions for each second along the 20-120 second span predictions &lt;- raw_gap_data |&gt; mutate(left = ifelse(left == &quot;Yes&quot;, 1, 0)) |&gt; group_by(species_common_name) |&gt; nest() |&gt; # Models and predictions mutate(model = map(.x = data, ~ smooth.spline(x = .$diff_time, y = .$left, df = 3)), pred = map(.x = model, ~ predict(., x = 20:120))) |&gt; select(species_common_name, pred) |&gt; unnest_wider(pred) |&gt; unnest(cols = c(x, y)) |&gt; rename(diff_time = x, pred = y) |&gt; ungroup() We can plot the model predictions for each value between the 20-120 second interval as follows (Figure 3.2). # Plot results for white-tailed deer wtd &lt;- predictions |&gt; filter(species_common_name == &quot;White-tailed Deer&quot;) raw_gap_data |&gt; mutate(left = ifelse(left == &quot;Yes&quot;, 1, 0)) |&gt; filter(species_common_name == &quot;White-tailed Deer&quot;) |&gt; ggplot(aes(x = diff_time, y = left, color = left)) + geom_jitter(height = 0.05, size = 2, alpha = 0.2) + scale_color_gradient(low = &quot;orange&quot;, high = &quot;darkgreen&quot;) + geom_line(data = wtd, aes(x = diff_time, y = pred), color = &quot;grey40&quot;, linewidth = 1.5) + theme_abmi() + # Custom ABMI theme scale_y_continuous(labels = c(&quot;Stayed&quot;, 0.2, 0.4, 0.6, 0.8, &quot;Left&quot;), breaks = seq(0, 1, 0.2)) + scale_x_continuous(breaks = seq(20, 120, by = 10), limits = c(20,120)) + labs(y = &quot;Probability of leaving field-of-view&quot;, x = &quot;Length of gap between images (seconds)&quot;) + theme(legend.position = &quot;none&quot;, axis.title.x = element_text(size = 12), axis.text.x = element_text(size = 11), axis.title.y = element_text(size = 11), axis.text.y = element_text(size = c(13, 11, 11, 11, 11, 13), face = c(&quot;bold&quot;, &quot;plain&quot;, &quot;plain&quot;, &quot;plain&quot;, &quot;plain&quot;, &quot;bold&quot;))) Figure 3.2: Probability of leaving the field-of-view based on gap length between subsequent images for white-tailed deer. "],["footnotes-and-citations.html", "4 Footnotes and citations 4.1 Footnotes 4.2 Citations", " 4 Footnotes and citations 4.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 1. 4.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2022) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References "],["blocks.html", "5 Blocks 5.1 Equations 5.2 Theorems and proofs 5.3 Callout blocks", " 5 Blocks 5.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{5.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (5.1). 5.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 5.1. Theorem 5.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 5.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["sharing-your-book.html", "6 Sharing your book 6.1 Publishing 6.2 404 pages 6.3 Metadata for sharing", " 6 Sharing your book 6.1 Publishing HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html 6.2 404 pages By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you’d like to customize your 404 page instead of using the default, you may add either a _404.Rmd or _404.md file to your project root and use code and/or Markdown syntax. 6.3 Metadata for sharing Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the index.Rmd YAML. To setup, set the url for your book and the path to your cover-image file. Your book’s title and description are also used. This gitbook uses the same social sharing data across all chapters in your book- all links shared will look the same. Specify your book’s source repository on GitHub using the edit key under the configuration options in the _output.yml file, which allows users to suggest an edit by linking to a chapter’s source file. Read more about the features of this output format here: https://pkgs.rstudio.com/bookdown/reference/gitbook.html Or use: ?bookdown::gitbook "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
