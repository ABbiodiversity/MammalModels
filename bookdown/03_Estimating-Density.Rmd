# Estimating Density

This chapter describes how the Time In Front of Camera (TIFC) method is used to calculate animal density using images collected from remote cameras. 

This method is also documented in [Becker et al (2022)](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecs2.4005).

```{r setup, include=FALSE}

# Attach packages for the chapter
library(ggbeeswarm)
library(abmi.themes)

add_abmi_fonts()

```


## Overview

### Simple Explanation

Density is the number of objects (trees, animals, etc.) per unit area. If a 100-m$^2$ plot contains one tree, the density is 1 tree/100-m$^2$, or 10,000 trees per km$^2$ ( Figure \@ref(fig:density)).  Similarly, if a camera has a field-of-view of 100-m$^2$ and there is always one animal in the field-of-view for the whole time that the camera is operating, the density of that species is 1 animal per 100-m$^2$, or 10,000 animals per km$^2$. It doesn’t matter if the animal is moving around within the field-of-view, as long as it stays in the field-of-view for the whole time. On the other hand, if that camera only has an animal in the field-of-view 1/10,000 of the time that it is operating, there is 1/10,000 animal per 100-m$^2$, or 1 animal per km-$^2$.  If the camera has two animals together for 1/10,000 of the time, this gives 2/10,000 animals per 100-m$^2$, or 2 animals per km-$^2$. This is how we use cameras to calculate density.

For a given density of animals, this simple measure is independent of home range sizes or movement rates. If home ranges were twice as big, they would have to overlap twice as much to maintain the same density. Therefore, an individual would be in a particular camera’s field-of-view half as often (because its home range is bigger – it has more other places to be), but there would be twice as many individuals at that camera. If movement rates were twice as fast, an individual would pass by the camera twice as often, but would spend half as much time in the field-of-view (because it is moving faster). For the simple example above, there would be two visits to the camera each occupying 1/20,000 of the time the camera is operating, rather than one visit for 1/10,000 of the time.  The other way of putting this is that only the total animal-time in the field-of-view matters, whether that comes from one long visit by one individual, several short visits by one individual, or several short visits each by a different individual. In all those cases, the density is the same; it is only the home range size and overlap and/or movement rates that are changing.

Two features of cameras require us to do some additional data processing to use this simple density measure: 

  1. **Cameras do not survey fixed areas, unlike quadrats**. The probability of an animal triggering the camera decreases with distance. We therefore have to estimate an *effective detection distance* (EDD) for the cameras, as is done for unlimited-distance point counts for birds or unlimited distance transect surveys. This effective distance can vary for different species, habitat types and time of year. 
  
  2. **Cameras take a series of images at discrete intervals, rather than providing a continuous record of how long an animal is in the field-of-view**. The discrete intervals need to be converted to a continuous measure to show how long the animal was in the field-of-view, accounting for the possibility that a moderately long interval between images might be from an animal present but not moving much, and therefore not triggering the camera, versus an animal that left the field-of-view and returned.

### Assumptions

There are a number of strong assumptions involved in using this measure to estimate density of a species. A couple big assumptions are: 

  + The cameras are a random or otherwise representative sample of the area. The density estimate applies to the field-of-view of the cameras. To make inferences about a larger region, the cameras need to be surveying a random or representative (e.g., systematic, systematic-random, random stratified) sample of the larger region. In particular, if cameras are intentionally placed in areas where species are more common, such as game trails, then the density estimate only applies to those places, not to a larger region.  
  
  + Animals are not attracted to or repelled by the cameras (or posts used to deploy the cameras, etc).  That also means that they do not spend more or less time in front of the camera because of the presence of the camera. The effect of lures or other attractants needs to be explicitly measured and accounted for. 
  
There are additional assumptions involved in the procedures to estimate effective detection distance, including an assumption that all animals within a certain distance of the camera are detected, and in converting the discrete images into time in field-of-view. These assumptions are discussed below. Because the world is complicated, assumptions are never met perfectly. The important thing is to consider – and, ideally, design auxiliary tests to measure – is whether the violations are serious enough to impact the answer to whatever question(s) the cameras are being used to answer. In many cases, absolute density estimates may not be accurate, but the results can still serve as a useable index of relative density, if assumptions are violated about equally in whatever units are being compared (habitat types, experimental treatments, years for long-term trend, etc).

A final consideration is the sampling distribution of density estimates. Because individual cameras sample tiny areas compared to the home ranges of the species they survey, the resulting sampling distribution can be horrendous – the majority of cameras never detect the species at all (density = 0), a few cameras record the species passing by once or twice for brief periods (low densities), and a very few number of cameras record long durations as the animals forage, rest, or play in front of the camera, or revisit a favourite spot repeatedly (very high densities). Longer duration camera deployments can help smooth out some of that extreme variation, but ultimately large numbers of cameras are required for precise estimates. Appropriate questions, rigorous study designs, and modest expectations are required for camera-based studies.

## Probabilistic gaps

From a pilot study, we determined that if there is a gap of less than 20 seconds between images of the same species at a camera, the animal is almost always still in the view (no evidence of it walking out and returning). Missing the odd time when it leaves the view for less than 20 seconds has little effect on estimates of the total time it is in the field-of-view. At the other end, if there is a gap of >120 seconds between images of the same species, this almost always represented animals leaving and then returning (i.e., the animal is seen walking out of the field-of-view, then walking back in). Gaps of 20-120 seconds are uncertain. These relatively long periods when the animal could be in the field-of-view or not are important when estimating the total duration animals are in the field-of-view, and thus density.

Using images from the 2015 ABMI Ecosystem Health project, we checked each 20-120 second gap in series’ of native mammals for evidence of the animal leaving and returning. We supplemented the 2015 data for less common species using data from 2016 and 2017. We looked at several images on either side of gaps of 20-120 seconds. In each sequence, the animal was designated as having left the field-of-view during the 20-120 second gap if there was clear evidence of it walking out of the field-of-view and then returning (or a different individual entering the field-of-view). If the animal stayed in one location within the field-of-view, or sequential images showed the animal in disconnected places (as often happens with smaller animals), the animal was assumed to have stayed.

Through this tagging procedure, we obtained the following training data to estimate the probability of an animal leaving during a gaps between 20 and 120 seconds between images. The `diff_time` variable refers to the number of seconds between images and the `left` variable refers to whether or not the individual left the camera field-of-view and returned ('Yes') or showed in the field-of-view for the duration of the gap ('No').  

```{r, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

raw_gap_data <- read_csv(paste0(g_drive, "data/processed/probabilistic-gaps/gap-leave-prob_raw-data_2021-10-05.csv")) |>
  mutate(left = ifelse(left == "1", "Yes", "No")) |>
  filter(!species_common_name == "Songbird") |>
  group_by(species_common_name) |>
  add_count() |>
  ungroup() |>
  mutate(rank = dense_rank(desc(n))) |>
  filter(rank < 13) |>
  mutate(species_common_name = fct_reorder(species_common_name, rank)) |>
  select(1, 3, 4)

```

```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}

head(raw_gap_data, 10)

```

Using White-tailed Deer as an example, we can visualize this data in Figure \@ref(fig:wtd). In general, observations where the animal stayed in the field of view are more closely clustered around the 20 second mark, and observations where the animal left the field of view are more evenly spread across the range of gap lengths.

```{r wtd, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, fig.showtext=TRUE, fig.align="left", fig.height=3, fig.cap="White-tailed Deer."}

# Plot of checked gaps for wtd
raw_gap_data |>
  mutate(left = as.factor(left)) |>
  filter(species_common_name == "White-tailed Deer") |>
  ggplot(aes(x = diff_time, y = left, color = left)) +
  geom_jitter(height = 0.15, size = 2, alpha = 0.2) +
  scale_color_manual(values = c("orange", "darkgreen")) +
  theme_abmi() + # Custom ABMI theme
  scale_y_discrete(labels = c("Stayed", "Left")) +
  scale_x_continuous(breaks = seq(20, 120, by = 10), limits = c(20,120)) +
  labs(y = "",
       x = "Length of gap between images (seconds)") +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 13, face = "bold"))

```

We used this data to develop models of the probability of a species leaving the field-of-view during a 20-120 second gap as a function of the gap duration. Smoothing splines were fit to the probability of leaving as a function of gap length, using a logit-linked binomial model.

```{r}

# Develop model for each species, then make predictions for each second along the 20-120 second span
predictions <- raw_gap_data |>
  mutate(left = ifelse(left == "Yes", 1, 0)) |>
  group_by(species_common_name) |>
  nest() |>
  # Models and predictions
  mutate(model = map(.x = data, ~ smooth.spline(x = .$diff_time, y = .$left, df = 3)),
         pred = map(.x = model, ~ predict(., x = 20:120))) |>
  select(species_common_name, pred) |>
  unnest_wider(pred) |>
  unnest(cols = c(x, y)) |>
  rename(diff_time = x, pred = y) |>
  ungroup()

```

We can plot the model predictions for each value between the 20-120 second interval as follows (Figure \@ref(fig:wtdpred)).

```{r wtdpred, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, fig.showtext=TRUE, fig.align="left", fig.height=5, fig.cap="Probability of leaving the field-of-view based on gap length between subsequent images for white-tailed deer."}

# Plot results for white-tailed deer
wtd <- predictions |> filter(species_common_name == "White-tailed Deer")

raw_gap_data |>
  mutate(left = ifelse(left == "Yes", 1, 0)) |>
  filter(species_common_name == "White-tailed Deer") |>
  ggplot(aes(x = diff_time, y = left, color = left)) +
  geom_jitter(height = 0.05, size = 2, alpha = 0.2) +
  scale_color_gradient(low = "orange", high = "darkgreen") +
  geom_line(data = wtd, aes(x = diff_time, y = pred), color = "grey40", linewidth = 1.5) +
  theme_abmi() + # Custom ABMI theme
  scale_y_continuous(labels = c("Stayed", 0.2, 0.4, 0.6, 0.8, "Left"), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(breaks = seq(20, 120, by = 10), limits = c(20,120)) +
  labs(y = "Probability of leaving field-of-view",
       x = "Length of gap between images (seconds)") +
  theme(legend.position = "none",
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        axis.text.y = element_text(size = c(13, 11, 11, 11, 11, 13),
                                   face = c("bold", "plain", "plain", "plain", "plain", "bold")))

```










