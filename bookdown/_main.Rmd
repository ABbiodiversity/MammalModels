--- 
title: "Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada"
author: "Marcus Becker, Dr. David J. Huggard, Dr. David Roberts"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- book.bib
- packages.bib
description: |
  ABMI Camera Methods and Mammal Modeling
link-citations: yes
github-repo: "ABbiodiversity/MammalModels"
---

# Introduction

The Alberta Biodiversity Monitoring Institute (ABMI) has developed a monitoring program for mammals within the province of Alberta, Canada. This technical document contains information describing how the image data from remote camera traps deployed across the province is collected, processed, and analyzed. 

This document will be updated as necessary to reflect changes in available data, deployment protocols, and modeling procedures. Readers are also encouraged to visit the ABMI's [Biodiversity Browser](https://www.abmi.ca/home/data-analytics/biobrowser-home) for current mammal species results.

## Acknowledgement

We would like to acknowledge that this work would not be possible without the dedication of the ABMI staff, both past and present. Field staff, geospatial experts, taxonomists, image taggers, and ecologists have all been integral components to this work. 

## Suggested Citation

Alberta Biodiversity Monitoring Institute. 2024. Using remote camera trap images to estimate densities and develop habitat models for mammals in Alberta, Canada. Alberta Biodiversity Monitoring Institute, Alberta, Canada. (https://abbiodiversity.github.io/MammalModels/).

## Contact

For all questions related to this document, the code therein, or acquiring raw image data, please contact Marcus Becker (marcus.becker@ualberta.ca). 

<!--chapter:end:index.Rmd-->

# Data Collection

## ABMI use of remote cameras

The ABMI has been using remote cameras to monitor mammal species in the province since 2014. 

As of `r Sys.Date()`, the ABMI has deployed X number of cameras across the province. 

## Camera deployment protocols

The ABMI's remote camera deployment protocols can be accessed here: [Terrestrial ABMI Autonomous Recording Unit (ARU) and Remote Camera Trap Protocols](https://abmi.ca/home/publications/551-600/599).

## List of ABMI camera projects

The following camera projects have been deployed by the ABMI:

Organize by year or type of project? Use gt package?

Note - would be nice to have a link to the WildTrax project for those projects that are published.

```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}

library(wildRtrax)
library(keyring)
library(tidyverse)
library(kableExtra)

Sys.setenv(WT_USERNAME = key_get("WT_USERNAME", keyring = "wildtrax"),
           WT_PASSWORD = key_get("WT_PASSWORD", keyring = "wildtrax"))

wt_auth()

abmi_projects <- wt_get_download_summary(sensor_id = "CAM") |>
  filter(organization == "ABMI",
         !str_detect(project, "ACME|Pilot|Elk Island|Hare Coat|Sahtu|Ticks|Compression|wildRtrax|first|Lemming|Zoo")) |>
  mutate(Project = str_remove(project, "^ABMI ")) |>
  select(Project, `No. Cameras` = tasks, Status = status)
  # Would it be better to arrange by year?
  # arrange(desc(Cameras))

kable(abmi_projects)

```


### Image Tagging

Image tagging is done on the [WildTrax](https://wildtrax.ca/about/) platform. Raw data is also publicly available from the ABMI via this platform.

The image tagging protocols used by the ABMI can be accessed here: 

There is a companion R package, [*wildRtrax*](https://abbiodiversity.github.io/wildRtrax/), which is also publicly available and contains many useful functions for downloading and analyzing camera data hosted on the platform. Throughout this document, references will be made to functions within this package.

Note: Would be nice to insert the wildRtrax package logo. 

## Species detections

The map below displays mammal species detections at ABMI Ecosystem Health (i.e., core grid) sites sampled with cameras between 2014 and 2022. Each site represented in the map below is sampled with four individual cameras placed 600-m apart in a square shape. Note that if a species is detected at at least one of the four cameras, the species is registered as an occurrence on the site map.   

(Note: Need to add EH 2023)

```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}

library(sf)
library(crosstalk) # Special branch of GH crosstalk repo required (filter-select-default)
library(leaflet)
library(leaflet.extras)
 
g_drive <- "G:/Shared drives/ABMI Camera Mammals/"

# Alberta provincial polygon
sf_ab <- st_read(paste0(g_drive, "data/spatial/ab_prov.shp"),
                 quiet = TRUE, stringsAsFactors = FALSE)

# Site locations
sites <- read_csv(paste0(g_drive, "data/lookup/climate/site-climate-summary_v2020.csv")) |>
  select(site = SITE_ID,
         latitude = PUBLIC_LATTITUDE,
         longitude = PUBLIC_LONGITUDE)

# Occurrence data
occ <- read_csv(paste0(g_drive, "Results/Occurrence/ABMI/occurrence_eh_site_2023-01-23.csv")) |>
  filter(!str_detect(site, "^W")) |>
  mutate(site = as.numeric(site))

# Species to display
sp <- occ |>
  group_by(common_name) |>
  summarise(n = sum(present)) |>
  filter(n >= 35,
         !str_detect(common_name, "Ermine|Squirrel"),
         !common_name == "Deer") |>
  select(common_name) |>
  pull()

occ_subset <- occ |>
  filter(common_name %in% sp) |>
  mutate(year = str_sub(project, start = -4, end = -1)) |>
  full_join(sites, by = "site") |>
  mutate(present = ifelse(is.na(present), "Not Surveyed", present)) |>
  mutate(present = factor(present, levels = c("Not Surveyed",
                                                 "TRUE",
                                                 "FALSE"))) |>
  select(site, latitude, longitude, year, common_name, present) |>
  arrange(site, year)

# Create SharedData object for crosstalk interactivity
shared_ss <- SharedData$new(occ_subset)

# Palette
pal <- colorFactor(palette = c("grey80",
                               "gold",
                               "black"),
                   domain = occ_subset$present)

# Create leaflet map
map <- sf_ab |>
  st_transform("+init=epsg:4326") %>%
  leaflet() %>%
  addTiles() %>%
  addProviderTiles("CartoDB.Positron") %>%
  addFullscreenControl() %>%
  addResetMapButton() %>%
  addScaleBar(position = "bottomright", 
              options = scaleBarOptions(imperial = FALSE)) |>
  
  addPolylines(color = "#0a0909", weight = 2.5, smoothFactor = 0.2, opacity = 3, fill = FALSE) |>
  
  addCircleMarkers(data = shared_ss,
                   color = ~pal(present), stroke = FALSE, fillOpacity = 1,
                   radius = 6) |>
  
  addLegend(data = shared_ss, pal = pal, values = ~present,
            position = "topright", opacity = 1, title = "Occurrence at Site:")

```

```{r, out.width="100%", fig.height=10, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}

# Create interactive filters
bscols(widths = c(3, 3),
       filter_checkbox("year", "Sampling Year:", shared_ss, ~year, columns = 2),
       filter_select("common_name", "Species:", shared_ss, ~common_name, multiple = FALSE, selected = "Badger"))

# Display map
map

```

## Deployment duration lengths

Here we can include all those figures. Yay. Do we really want that?






<!--chapter:end:02_Data-Collection.Rmd-->

# Estimating Density

This chapter describes how the Time In Front of Camera (TIFC) method is used to calculate animal density using images collected from remote cameras. 

This method is also documented in [Becker et al (2022)](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecs2.4005).

## Overview

### Simple Explanation

Density is the number of objects (trees, animals, etc.) per unit area. If a 100-m$^2$ plot contains one tree, the density is 1 tree/100-m$^2$, or 10,000 trees per km$^2$ ( Figure \@ref(fig:density)).  Similarly, if a camera has a field-of-view of 100-m$^2$ and there is always one animal in the field-of-view for the whole time that the camera is operating, the density of that species is 1 animal per 100-m$^2$, or 10,000 animals per km$^2$. It doesn’t matter if the animal is moving around within the field-of-view, as long as it stays in the field-of-view for the whole time. On the other hand, if that camera only has an animal in the field-of-view 1/10,000 of the time that it is operating, there is 1/10,000 animal per 100-m$^2$, or 1 animal per km-$^2$.  If the camera has two animals together for 1/10,000 of the time, this gives 2/10,000 animals per 100-m$^2$, or 2 animals per km-$^2$. This is how we use cameras to calculate density.

For a given density of animals, this simple measure is independent of home range sizes or movement rates. If home ranges were twice as big, they would have to overlap twice as much to maintain the same density. Therefore, an individual would be in a particular camera’s field-of-view half as often (because its home range is bigger – it has more other places to be), but there would be twice as many individuals at that camera. If movement rates were twice as fast, an individual would pass by the camera twice as often, but would spend half as much time in the field-of-view (because it is moving faster). For the simple example above, there would be two visits to the camera each occupying 1/20,000 of the time the camera is operating, rather than one visit for 1/10,000 of the time.  The other way of putting this is that only the total animal-time in the field-of-view matters, whether that comes from one long visit by one individual, several short visits by one individual, or several short visits each by a different individual. In all those cases, the density is the same; it is only the home range size and overlap and/or movement rates that are changing.

Two features of cameras require us to do some additional data processing to use this simple density measure: 

  1. **Cameras do not survey fixed areas, unlike quadrats**. The probability of an animal triggering the camera decreases with distance. We therefore have to estimate an *effective detection distance* (EDD) for the cameras, as is done for unlimited-distance point counts for birds or unlimited distance transect surveys. This effective distance can vary for different species, habitat types and time of year. 
  
  2. **Cameras take a series of images at discrete intervals, rather than providing a continuous record of how long an animal is in the field-of-view**. The discrete intervals need to be converted to a continuous measure to show how long the animal was in the field-of-view, accounting for the possibility that a moderately long interval between images might be from an animal present but not moving much, and therefore not triggering the camera, versus an animal that left the field-of-view and returned.

### Assumptions

There are a number of strong assumptions involved in using this measure to estimate density of a species. A couple big assumptions are: 

  + The cameras are a random or otherwise representative sample of the area. The density estimate applies to the field-of-view of the cameras. To make inferences about a larger region, the cameras need to be surveying a random or representative (e.g., systematic, systematic-random, random stratified) sample of the larger region. In particular, if cameras are intentionally placed in areas where species are more common, such as game trails, then the density estimate only applies to those places, not to a larger region.  
  
  + Animals are not attracted to or repelled by the cameras (or posts used to deploy the cameras, etc).  That also means that they do not spend more or less time in front of the camera because of the presence of the camera. The effect of lures or other attractants needs to be explicitly measured and accounted for. 
  
There are additional assumptions involved in the procedures to estimate effective detection distance, including an assumption that all animals within a certain distance of the camera are detected, and in converting the discrete images into time in field-of-view. These assumptions are discussed below. Because the world is complicated, assumptions are never met perfectly. The important thing is to consider – and, ideally, design auxiliary tests to measure – is whether the violations are serious enough to impact the answer to whatever question(s) the cameras are being used to answer. In many cases, absolute density estimates may not be accurate, but the results can still serve as a useable index of relative density, if assumptions are violated about equally in whatever units are being compared (habitat types, experimental treatments, years for long-term trend, etc).

A final consideration is the sampling distribution of density estimates. Because individual cameras sample tiny areas compared to the home ranges of the species they survey, the resulting sampling distribution can be horrendous – the majority of cameras never detect the species at all (density = 0), a few cameras record the species passing by once or twice for brief periods (low densities), and a very few number of cameras record long durations as the animals forage, rest, or play in front of the camera, or revisit a favourite spot repeatedly (very high densities). Longer duration camera deployments can help smooth out some of that extreme variation, but ultimately large numbers of cameras are required for precise estimates. Appropriate questions, rigorous study designs, and modest expectations are required for camera-based studies.

## Probabilistic gaps

From a pilot study, we determined that if there is a gap of less than 20 seconds between images of the same species at a camera, the animal is almost always still in the view (no evidence of it walking out and returning). Missing the odd time when it leaves the view for less than 20 seconds has little effect on estimates of the total time it is in the field-of-view. At the other end, if there is a gap of >120 seconds between images of the same species, this almost always represented animals leaving and then returning (i.e., the animal is seen walking out of the field-of-view, then walking back in). Gaps of 20-120 seconds are uncertain. These relatively long periods when the animal could be in the field-of-view or not are important when estimating the total duration animals are in the field-of-view, and thus density.

Using images from the 2015 ABMI Ecosystem Health project, we checked each 20-120 second gap in series’ of native mammals for evidence of the animal leaving and returning. We supplemented the 2015 data for less common species using data from 2016 and 2017. We looked at several images on either side of gaps of 20-120 seconds. In each sequence, the animal was designated as having left the field-of-view during the 20-120 second gap if there was clear evidence of it walking out of the field-of-view and then returning (or a different individual entering the field-of-view). If the animal stayed in one location within the field-of-view, or sequential images showed the animal in disconnected places (as often happens with smaller animals), the animal was assumed to have stayed.

Through this tagging procedure, we obtained the following training data to estimate the probability of an animal leaving during a gaps between 20 and 120 seconds between images. The `diff_time` variable refers to the number of seconds between images and the `left` variable refers to whether or not the individual left the camera field-of-view and returned ('Yes') or showed in the field-of-view for the duration of the gap ('No').  

```{r, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

raw_gap_data <- read_csv(paste0(g_drive, "data/processed/probabilistic-gaps/gap-leave-prob_raw-data_2021-10-05.csv")) |>
  mutate(left = ifelse(left == "1", "Yes", "No")) |>
  filter(!species_common_name == "Songbird") |>
  group_by(species_common_name) |>
  add_count() |>
  ungroup() |>
  mutate(rank = dense_rank(desc(n))) |>
  filter(rank < 13) |>
  mutate(species_common_name = fct_reorder(species_common_name, rank)) |>
  select(1, 3, 4)

```

```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}

head(raw_gap_data, 10)

```

We can visualize this data in Figure \@ref(fig:gap). In general, observations where the animal stayed in the field of view are more closely clustered around the 20 second mark, and observations where the animal left the field of view are more evenly spread across the range of gap lengths.

```{r gap, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, fig.showtext=TRUE, fig.align="left", fig.height=9}

library(ggplot2)
# abmi.themes is a custom R package for ABMI-themed ggplots
library(abmi.themes)
# Add Montserrat font
add_abmi_fonts()

# Plot of checked gaps
raw_gap_data |>
  mutate(left = as.factor(left)) %>%
  ggplot(aes(x = diff_time, y = left, color = species_common_name)) +
  geom_beeswarm(cex = 0.8) +
  facet_wrap(~ species_common_name, nrow = 4) +
  scale_color_viridis_d(direction = -1) +
  theme_abmi() + # Custom ABMI theme
  scale_y_discrete(labels = c("Stayed", "Left")) +
  scale_x_continuous(breaks = seq(20, 120, by = 20), limits = c(20,120)) +
  labs(y = "",
       x = "Gap Length (s)",
       title = "Did the animal leave the camera field of view?") +
  theme(legend.position = "none",
        strip.text = element_text(size = 10))

```










<!--chapter:end:03_Estimating-Density.Rmd-->

# Footnotes and citations 

## Footnotes

Footnotes are put inside the square brackets after a caret `^[]`. Like this one ^[This is a footnote.]. 

## Citations

Reference items in your bibliography file(s) using `@key`.

For example, we are using the **bookdown** package [@R-bookdown] (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and **knitr** [@xie2015] (this citation was added manually in an external file book.bib). 
Note that the `.bib` files need to be listed in the index.Rmd with the YAML `bibliography` key.


The RStudio Visual Markdown Editor can also make it easier to insert citations: <https://rstudio.github.io/visual-markdown-editing/#/citations>

<!--chapter:end:04-citations.Rmd-->

# Blocks

## Equations

Here is an equation.

\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

You may refer to using `\@ref(eq:binom)`, like see Equation \@ref(eq:binom).


## Theorems and proofs

Labeled theorems can be referenced in text using `\@ref(thm:tri)`, for example, check out this smart theorem \@ref(thm:tri).

::: {.theorem #tri}
For a right triangle, if $c$ denotes the *length* of the hypotenuse
and $a$ and $b$ denote the lengths of the **other** two sides, we have
$$a^2 + b^2 = c^2$$
:::

Read more here <https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html>.

## Callout blocks


The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html

<!--chapter:end:05-blocks.Rmd-->

# Sharing your book

## Publishing

HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html

## 404 pages

By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you'd like to customize your 404 page instead of using the default, you may add either a `_404.Rmd` or `_404.md` file to your project root and use code and/or Markdown syntax.

## Metadata for sharing

Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the `index.Rmd` YAML. To setup, set the `url` for your book and the path to your `cover-image` file. Your book's `title` and `description` are also used.



This `gitbook` uses the same social sharing data across all chapters in your book- all links shared will look the same.

Specify your book's source repository on GitHub using the `edit` key under the configuration options in the `_output.yml` file, which allows users to suggest an edit by linking to a chapter's source file. 

Read more about the features of this output format here:

https://pkgs.rstudio.com/bookdown/reference/gitbook.html

Or use:

```{r eval=FALSE}
?bookdown::gitbook
```



<!--chapter:end:06-share.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

